{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc603094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11976d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')   #Instantiates an instance of the environment \n",
    "env.reset() #Resets the environment to an initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**state**: is a array([position,velocity])\n",
    "\n",
    "**Initial state**:\n",
    "* _velocity_ : The starting velocity of the car is always assigned to 0\n",
    "* _position_ : The starting position of the car is assigned a uniform random value in [-0.6 , -0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for q learning\n",
    "epsilon = 0.8\n",
    "discount = 0.9\n",
    "learning_rate = 0.2 \n",
    "episodes = 5000 # number of episodes\n",
    "\n",
    "reduction = epsilon/episodes  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track rewards\n",
    "reward_list = []\n",
    "all_reward_list = []\n",
    "ave_reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine size of discretized state space\n",
    "states = (env.observation_space.high - env.observation_space.low) *\\\n",
    "        np.array([10, 100])\n",
    "states = np.round(states, 0).astype(int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6689f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q table\n",
    "q_table = np.random.uniform(\n",
    "                low=-1, \n",
    "                high=1,\n",
    "                size=(states[0], states[1],env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ae392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_rewards(total_reward,i,reward_list,all_reward_list,ave_reward_list):\n",
    "        reward_list.append(total_reward)\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            all_reward_list.extend(reward_list)\n",
    "            reward_list = []\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20348ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(epsilon,discount,learning_rate):\n",
    "    for i in range(episodes):\n",
    "        done = False # game is not over yet \n",
    "        total_reward, reward = 0,0 \n",
    "        state = env.reset() #Resets the environment to an initial state\n",
    "        \n",
    "         # Discretize initial state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "        while done != True:\n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(q_table[state_adj[0], state_adj[1]])\n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done,info = env.step(action)\n",
    "\n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low) * \\\n",
    "                np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "                \n",
    "            if done and state2[0]>= 0.5:# Allow for terminal states\n",
    "                q_table[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            else: # Adjust q_table value for current state\n",
    "                delta = learning_rate*(reward +\n",
    "                                  discount*np.max(q_table[state2_adj[0],\n",
    "                                                    state2_adj[1]]) -\n",
    "                                  q_table[state_adj[0], state_adj[1], action])\n",
    "                q_table[state_adj[0], state_adj[1], action] += delta\n",
    "            # Update variables\n",
    "            total_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        # Decay epsilon\n",
    "        if epsilon > 0:\n",
    "            epsilon -= reduction\n",
    "            \n",
    "        track_rewards(total_reward,i,reward_list,all_reward_list,ave_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_episode_render(policy):\n",
    "    observation= env.reset()\n",
    "    done = False\n",
    "    while done != True:\n",
    "        state_adj = (observation - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        action = policy[state_adj[0]][state_adj[1]]\n",
    "        \n",
    "        env.render()\n",
    "        # proceed environment for each step\n",
    "        # get observation, reward and done after each step\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-learning algorithm ****\n",
    "QLearning(epsilon,discount,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7095849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render best episode\n",
    "policy=np.argmax(q_table, axis=2)\n",
    "best_episode_render(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ebb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ave Rewards\n",
    "plt.plot(100*(np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('Average rewards.jpg') \n",
    "plt.show()    \n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
